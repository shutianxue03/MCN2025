{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import scipy.io as sio  # For saving\n",
    "\n",
    "# For clearing cache\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "from one.api import ONE\n",
    "from brainbox.io.one import SpikeSortingLoader, SessionLoader\n",
    "from brainbox.behavior.training import compute_performance\n",
    "from brainbox.population.decode import get_spike_counts_in_bins\n",
    "\n",
    "# Connect to IBL data server\n",
    "one = ONE(base_url='https://openalyx.internationalbrainlab.org')\n",
    "print(one.cache_dir)\n",
    "\n",
    "# =========================================\n",
    "# ðŸ” Loop through contrast Ã— prior settings\n",
    "# =========================================\n",
    "# Spike count window settings\n",
    "window_start = 0          # in seconds\n",
    "window_end = 300 / 1000   # in seconds\n",
    "\n",
    "nNeurons_lb = 10\n",
    "nTrials_lb = 20\n",
    "nTrials_lb = 2\n",
    "RT_ub = 2000 # trials with RT more than this dur (in ms) will be removed\n",
    "\n",
    "# # Define experiment(s)\n",
    "# eid = '7af49c00-63dd-4fed-b2e0-1b3bd945b20b'\n",
    "# eids = [eid]  # Can add more eids here\n",
    "\n",
    "# Find spike-sorted sessions from Churchland lab\n",
    "# lab='churchlandlab',\n",
    "lab='churchlandlab'\n",
    "eids = one.search(\n",
    "    lab=lab,\n",
    "    dataset_types=['spikes.times']\n",
    ")\n",
    "print(f\">>> {lab} sessions with spike sorting: {len(eids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fdc4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "process = psutil.Process()\n",
    "mem_info = process.memory_info()\n",
    "print(f\"Memory used: {mem_info.rss / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994979ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example eid for debugging\n",
    "# eid = 'aad23144-0e52-4eac-80c5-c4ee2decb198'\n",
    "# pids, labels = one.eid2pid(eid)\n",
    "# trials = one.load_object(eid, 'trials', collection='alf')\n",
    "# contrast_levels = np.unique(trials['contrastLeft'])\n",
    "# prior_levels = np.unique(trials['probabilityLeft'])\n",
    "process = psutil.Process()\n",
    "mem_info = process.memory_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cccc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over sessions (EIDs)\n",
    "for indEid, eid in enumerate(eids, start=1):\n",
    "\n",
    "    # ================================\n",
    "    # ðŸ“¦ Load trial and probe info\n",
    "    # ================================\n",
    "    pids, labels = one.eid2pid(eid)\n",
    "    trials = one.load_object(eid, 'trials', collection='alf')\n",
    "    contrast_levels = np.unique(trials['contrastLeft'])\n",
    "    prior_levels = np.unique(trials['probabilityLeft'])\n",
    "\n",
    "    # assert that contrast_levels has 5 levels: 0, .0625, .125, .25, .5, 1\n",
    "    # assert np.array_equal(contrast_levels, [0., 0.0625, 0.125, 0.25, 0.5, 1]), f\"Unexpected contrast_levels: {contrast_levels}\"\n",
    "    \n",
    "    # assert that prior_levels has three levels: 0.2, 0.5 and 0.8\n",
    "    assert np.array_equal(prior_levels, [0.2, 0.5, 0.8]), f\"Unexpected prior_levels: {prior_levels}\"\n",
    "\n",
    "    for label in labels:\n",
    "        print(f\"\\nðŸ“‚ Session #{indEid}/{len(eids)}: {eid} | Probe: {label}\")\n",
    "\n",
    "        # ================================\n",
    "        # ðŸ”¬ Load spikes and clusters\n",
    "        # ================================\n",
    "        ssl = SpikeSortingLoader(eid=eid, one=one, pname=label)\n",
    "        spikes, clusters, channels = ssl.load_spike_sorting()\n",
    "        clusters = ssl.merge_clusters(spikes, clusters, channels)\n",
    "        regions_all = np.unique(clusters['acronym'])\n",
    "\n",
    "        for target_region in regions_all:\n",
    "            for flag_good_quality in [False, True]:  # True = label==1\n",
    "\n",
    "                # ================================\n",
    "                # ðŸ§  Neuron selection\n",
    "                # ================================\n",
    "\n",
    "                # Select neurons that have matched region and quality\n",
    "                region_mask = clusters['acronym'] == target_region\n",
    "                n_neurons = len(clusters['acronym'])\n",
    "                quality_mask = clusters['label'] == 1 if flag_good_quality else np.ones(n_neurons, dtype=bool)\n",
    "                valid_cluster_mask = region_mask & quality_mask\n",
    "                \n",
    "                # Apply valid-neuron mask\n",
    "                valid_cluster_ids = clusters['cluster_id'][valid_cluster_mask]\n",
    "                del region_mask, n_neurons, quality_mask, valid_cluster_mask\n",
    "                gc.collect()\n",
    "\n",
    "                if len(valid_cluster_ids) > nNeurons_lb:\n",
    "                    process = psutil.Process()\n",
    "                    mem_info = process.memory_info()\n",
    "                    print(f\"ðŸŽ¯ Region: {target_region}; Quality: {flag_good_quality} | Neurons: {len(valid_cluster_ids)}\")\n",
    "\n",
    "                    for contrast in contrast_levels:\n",
    "                        for prior in prior_levels:\n",
    "\n",
    "                            # ================================\n",
    "                            # ðŸ§ª Trial selection\n",
    "                            # ================================\n",
    "                            # Select trials with matched contrast and prior\n",
    "                            trial_mask = (trials['contrastLeft'] == contrast) & \\\n",
    "                                         (trials['probabilityLeft'] == prior)\n",
    "\n",
    "                            # Select trials with RT within RT_ub\n",
    "                            rt = trials['response_times'] - trials['stimOn_times']\n",
    "                            trial_mask &= (rt <= RT_ub) & ~np.isnan(rt)\n",
    "\n",
    "                            # Select trials with long enough latency between stim onset and movement onset\n",
    "                            movement_latency = trials['firstMovement_times'] - trials['stimOn_times']\n",
    "                            no_early_movement_mask = (movement_latency >= window_end) | np.isnan(movement_latency)\n",
    "                            trial_mask &= no_early_movement_mask\n",
    "\n",
    "                            # Check priors\n",
    "                            n_total = np.sum((trials['contrastLeft'] == contrast) & (trials['probabilityLeft'] == prior))\n",
    "                            # print(f\"ðŸ“Š Prior={prior}, Contrast={contrast} | Total trials: {n_total}\")\n",
    "\n",
    "                            # Apply valid-trial mask\n",
    "                            valid_trial_times = trials['stimOn_times'][trial_mask]\n",
    "\n",
    "                            # check priors again\n",
    "                            n_after = np.sum(trial_mask)\n",
    "                            # print(f\"   â†³ Trials after filtering: {n_after}\")\n",
    "\n",
    "                            del trial_mask, movement_latency, no_early_movement_mask\n",
    "                            gc.collect()\n",
    "\n",
    "\n",
    "                            if len(valid_trial_times) > nTrials_lb:\n",
    "                                process = psutil.Process()\n",
    "                                mem_info = process.memory_info()\n",
    "                                print(f\"  âœ…Contrast: {contrast} | Prior: {prior} | Trials: {len(valid_trial_times)}\")\n",
    "\n",
    "                    del valid_cluster_ids\n",
    "                    gc.collect()\n",
    "\n",
    "        del spikes, clusters, channels, ssl\n",
    "        gc.collect()\n",
    "\n",
    "    del trials\n",
    "    gc.collect()\n",
    "\n",
    "    # Clear cache\n",
    "    # session_cache_path = os.path.join(one.cache_dir, eid)\n",
    "    session_cache_path = os.path.join(one.cache_dir, str(eid))\n",
    "    \n",
    "    if os.path.exists(session_cache_path):\n",
    "        shutil.rmtree(session_cache_path)\n",
    "        print(f\"ðŸ§¹ Cleared cache for session {eid}\")\n",
    "\n",
    "del pids, eids, labels\n",
    "print('âœ… DONE')\n",
    "\n",
    "# Clear all cache at the end\n",
    "shutil.rmtree(one.cache_dir)\n",
    "print(\"ðŸ§¹ All cache cleared.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c38a418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
